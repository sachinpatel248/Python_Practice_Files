{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "c:\\program files\\python36\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import string\n",
    "from keras.models import Sequential, load_model\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "from nltk.tag import pos_tag\n",
    "import json\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERAS_Alphabet_model_File_Path = r'./Keras_Alphabet_Model.h5'\n",
    "Alphabet_Mapping_List = list(string.ascii_uppercase)\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "\n",
    "global X, Y, W, H, temp_Index, Predicted_Text, words_To_Hide, list_Character_Positions, list_Words_To_Hide, word_Matching_Thresh_Value\n",
    "\n",
    "\n",
    "X=Y=W=H=0\n",
    "temp_Index = 0\n",
    "Predicted_Text = ''\n",
    "words_To_Hide = ''\n",
    "list_Character_Positions = []\n",
    "list_Words_To_Hide = []\n",
    "word_Matching_Thresh_Value = 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    KERAS_Alphabet_Model = load_model(KERAS_Alphabet_model_File_Path)\n",
    "except:\n",
    "    KERAS_Model = load_model(KERAS_model_File_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Processed_Image(input_Image):\n",
    "    #Gray Image\n",
    "    input_Image = cv2.cvtColor(input_Image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #Blurring\n",
    "    input_Image = cv2.GaussianBlur(input_Image,(5,5),0)\n",
    "    \n",
    "    #Otsu_Threshold_GaussianBlur_Image\n",
    "    input_Image = cv2.threshold(input_Image,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    #Take Negative\n",
    "    input_Image = cv2.bitwise_not(input_Image)\n",
    "\n",
    "    #Closing To Remove Noise\n",
    "    input_Image = cv2.morphologyEx(input_Image, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    return input_Image\n",
    "\n",
    "\n",
    "\n",
    "## Dilation for thickening image\n",
    "def Get_Dilated_Image(input_Image, number):\n",
    "    kernel = np.ones((5,number),np.uint8)\n",
    "    \n",
    "    return (cv2.dilate(input_Image, kernel, iterations=3))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "def Get_Countours(input_Image):\n",
    "    return  (cv2.findContours(input_Image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[1])\n",
    "\n",
    "    return contours\n",
    "\n",
    "\n",
    "def Sort_Countours(Contours, method=\"left-to-right\"):\n",
    "    # initialize the reverse flag and sort index\n",
    "    reverse = False\n",
    "    i = 0\n",
    "\n",
    "    # handle if we need to sort in reverse\n",
    "    if method == \"right-to-left\" or method == \"bottom-to-top\":\n",
    "        reverse = True\n",
    "\n",
    "    # handle if we are sorting against the y-coordinate rather than\n",
    "    # the x-coordinate of the bounding box\n",
    "    if method == \"top-to-bottom\" or method == \"bottom-to-top\":\n",
    "        i = 1\n",
    "\n",
    "    # construct the list of bounding boxes and sort them from top to\n",
    "    # bottom\n",
    "    boundingBoxes = [cv2.boundingRect(c) for c in Contours]\n",
    "    (Contours, boundingBoxes) = zip(*sorted(zip(Contours, boundingBoxes),\n",
    "                                        key=lambda b:b[1][i], reverse=reverse))\n",
    "\n",
    "    # return the list of sorted contours and bounding boxes\n",
    "    return (Contours)\n",
    "\n",
    "def getNewResizedImage(input_Image, Image_Size):\n",
    "    height,width = input_Image.shape\n",
    "    #print (height, width)\n",
    "\n",
    "    if width > height:\n",
    "        aspect_Ratio = (float)(width/height)\n",
    "        width = 20\n",
    "        height = round(width/aspect_Ratio)\n",
    "    else:\n",
    "        aspect_Ratio = (float)(height/width)\n",
    "        height = 20\n",
    "        width = round(height/aspect_Ratio)\n",
    "        \n",
    "    input_Image = cv2.resize(input_Image, (width,height), interpolation = cv2.INTER_AREA )\n",
    "    \n",
    "    height,width = input_Image.shape\n",
    "    \n",
    "    number_Of_Column_To_Add = 28-width\n",
    "    temp_Column = np.zeros( (height , int(number_Of_Column_To_Add/2)), dtype = np.uint8)\n",
    "    input_Image = np.append(temp_Column, input_Image, axis=1)\n",
    "    input_Image = np.append(input_Image, temp_Column, axis=1)\n",
    "\n",
    "\n",
    "    height,width = input_Image.shape\n",
    "\n",
    "    number_Of_Row_To_Add = 28-height\n",
    "    temp_Row= np.zeros( (int(number_Of_Row_To_Add/2) , width ), dtype = np.uint8)\n",
    "    input_Image = np.concatenate((temp_Row,input_Image))\n",
    "    input_Image = np.concatenate((input_Image,temp_Row))\n",
    "\n",
    "    return cv2.resize(input_Image, (Image_Size,Image_Size), interpolation = cv2.INTER_AREA )\n",
    "\n",
    "\n",
    "def Get_Average_Space_From_Image_Line(contours):\n",
    "    total_space = 0\n",
    "    number_Of_Character = 0\n",
    "    last_Point = -1\n",
    "    avg_Space = 0\n",
    "    for cnt in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "\n",
    "        if w > 30 and h > 30 and x > 0 and y > 0:\n",
    "            if last_Point == -1:\n",
    "                last_Point = 0\n",
    "            else:\n",
    "                total_space = total_space + ( x - last_Point )\n",
    "            last_Point = x + w\n",
    "\n",
    "            number_Of_Character = number_Of_Character + 1\n",
    "\n",
    "    avg_Space = int (total_space/number_Of_Character)\n",
    "\n",
    "    total_space = 0\n",
    "    last_Point = -1\n",
    "    number_Of_Character = 0\n",
    "\n",
    "##    for cnt in contours:\n",
    "##        x,y,w,h = cv2.boundingRect(cnt)\n",
    "##\n",
    "##        if w > 30 and h > 30 and x > 0 and y > 0:\n",
    "##            if last_Point == -1:\n",
    "##                last_Point = 0\n",
    "##            else:\n",
    "##                space = ( (x + w) - last_Point )\n",
    "##                if space < avg_Space:\n",
    "##                    total_space = total_space + space\n",
    "##                    number_Of_Character = number_Of_Character + 1\n",
    "##            last_Point = x + w\n",
    "##            \n",
    "##    avg_Space = int (total_space/number_Of_Character)\n",
    "\n",
    "    return avg_Space\n",
    "\n",
    "\n",
    "\n",
    "def Get_Text_From_Image(Image, contours):\n",
    "    global count,X,Y,W,H, list_Character_Positions\n",
    "    KERAS_Alphabet_Prediction = ''\n",
    "\n",
    "    total_space = 0\n",
    "    number_Of_Character = 0\n",
    "    last_Point = -1\n",
    "\n",
    "    #avg_Space = Get_Average_Space_From_Image_Line(contours)\n",
    "    last_Point = -1\n",
    "    total_space = 0\n",
    "    #print (avg_Space)\n",
    "    \n",
    "    Word_Dilated_Image = Get_Dilated_Image(Image,20)\n",
    "    Word_Contours = Get_Countours(Word_Dilated_Image)\n",
    "    Word_Contours = Sort_Countours(Word_Contours,\"left-to-right\")\n",
    "    last_Word_Contour_Index = 0\n",
    "    Word_X, Word_Y, Word_W, Word_H = cv2.boundingRect(Word_Contours[last_Word_Contour_Index])\n",
    "    last_Word_Contour_Max_X_Range = Word_X + Word_W\n",
    "    # i.e X + W\n",
    "            \n",
    "\n",
    "    for cnt in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        give_Space = False\n",
    "        #print (x,y,w,h)\n",
    "\n",
    "        # Reject if Contour is not of desired size (too small)\n",
    "        if w > 30 and h > 30 and x > 0 and y > 0:\n",
    "\n",
    "#             ## Spacing based on average space between character\n",
    "#             if last_Point == -1:\n",
    "#                 last_Point = 0\n",
    "#             else:\n",
    "#                 total_space = ( x - last_Point )\n",
    "\n",
    "#             last_Point = x + w\n",
    "\n",
    "#             if total_space > (avg_Space*1.3):\n",
    "#                 give_Space = True\n",
    "            \n",
    "            ## Spacing based on word formation\n",
    "            if x > last_Word_Contour_Max_X_Range:\n",
    "                give_Space = True\n",
    "                last_Word_Contour_Index = last_Word_Contour_Index + 1\n",
    "                Word_X, Word_Y, Word_W, Word_H = cv2.boundingRect(Word_Contours[last_Word_Contour_Index])\n",
    "                last_Word_Contour_Max_X_Range = Word_X + Word_W\n",
    "                # i.e X + W\n",
    "                \n",
    "            if give_Space == True:\n",
    "                KERAS_Alphabet_Prediction = KERAS_Alphabet_Prediction + \" \"\n",
    "                list_Character_Positions.append((-1,-1,-1,-1,\" \"))\n",
    "                \n",
    "\n",
    "            resize_image = getNewResizedImage(Image[y:y+h, x:x+w] , 28)\n",
    "            resize_image = resize_image.flatten()\n",
    "\n",
    "            temp_Index = int(KERAS_Alphabet_Model.predict_classes(resize_image.reshape(1,784)/255.0)[0])\n",
    "            alphabet_probability = (KERAS_Alphabet_Model.predict_proba(resize_image.reshape(1,784)/255.0))\n",
    "            sort_alphabet_probability = -np.sort(-alphabet_probability)\n",
    "\n",
    "            KERAS_Alphabet_Prediction = KERAS_Alphabet_Prediction + Alphabet_Mapping_List[int(temp_Index)]\n",
    "\n",
    "            list_Character_Positions.append((x+X,y+Y,w,h,str(Alphabet_Mapping_List[int(temp_Index)])))\n",
    "\n",
    "##            if sort_alphabet_probability[0,0] > 0.99:\n",
    "##                KERAS_Alphabet_Prediction = KERAS_Alphabet_Prediction + Alphabet_Mapping_List[int(temp_Index)]\n",
    "##            else:\n",
    "##                alternate_Probable_Alphabet = Alphabet_Mapping_List[int(np.where(alphabet_probability == sort_alphabet_probability[0,1])[1][0])]\n",
    "##                KERAS_Alphabet_Prediction = KERAS_Alphabet_Prediction + \"[\" + Alphabet_Mapping_List[int(temp_Index)] + \",\" + alternate_Probable_Alphabet + \"]\"\n",
    "\n",
    "    list_Character_Positions.append((-1,-1,-1,-1,\" \"))\n",
    "\n",
    "    return KERAS_Alphabet_Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Similar_List_Of_Words_To_Hide(list_Words_To_Hide, Predicted_Text):\n",
    "    list_Similar_Words = []\n",
    "    list_Predicted_Text_Words = Predicted_Text.split(\" \")\n",
    "    global word_Matching_Thresh_Value\n",
    "    \n",
    "    for word_To_Hide in list_Words_To_Hide:\n",
    "        for predicted_Word in list_Predicted_Text_Words:\n",
    "            word_Probability = fuzz.ratio(word_To_Hide,predicted_Word)\n",
    "            if word_Probability > word_Matching_Thresh_Value:\n",
    "                list_Similar_Words.append(predicted_Word)\n",
    "                \n",
    "    return list_Similar_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_List_Dimension_Of_Words_Hide(Predicted_Text, list_Character_Positions,  list_Words_To_Hide):\n",
    "    list_Dimension_Of_Words_Hide = []\n",
    "    \n",
    "    for word in list_Words_To_Hide:\n",
    "        count_Of_Word = Predicted_Text.count(word)\n",
    "        word_index = -1\n",
    "        char_index = 0\n",
    "        \n",
    "        for index in range(0,count_Of_Word):\n",
    "            word_index = Predicted_Text.find(word, word_index + 1)\n",
    "            Min_X = Min_Y = 1000000\n",
    "            Max_X = Max_Y = 0\n",
    "            \n",
    "            for i in range (word_index, word_index + len(word)):\n",
    "                char_Pos = list_Character_Positions[i]\n",
    "                x,y,w,h = char_Pos[0],char_Pos[1],char_Pos[2],char_Pos[3]\n",
    "                \n",
    "                if Min_X > x and x > -1:\n",
    "                    Min_X = x\n",
    "                if Min_Y > y and y > -1:\n",
    "                    Min_Y = y\n",
    "                if Max_X < (x + w):\n",
    "                    Max_X = (x + w)\n",
    "                if Max_Y < (y + h):\n",
    "                    Max_Y = (y + h)\n",
    "                    \n",
    "                char_index = char_index + 1\n",
    "            \n",
    "            list_Dimension_Of_Words_Hide.append((Min_X, Min_Y, Max_X, Max_Y, word))\n",
    "                \n",
    "    return list_Dimension_Of_Words_Hide\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Character_Position_From_Image(image_File_Path, Words_To_Hide = \"\"):\n",
    "    if not (os.path.isfile(image_File_Path)):\n",
    "        return \"Invalid image file path\"\n",
    "    \n",
    "    image = cv2.imread(image_File_Path)\n",
    "    image = Get_Processed_Image(image)\n",
    "    \n",
    "    image_With_Lines = Get_Dilated_Image(image, 60)\n",
    "    \n",
    "    contours = Get_Countours(image_With_Lines)\n",
    "    contours = Sort_Countours(contours, \"top-to-bottom\")\n",
    "    \n",
    "    global X, Y, W, H, temp_Index, Predicted_Text, words_To_Hide, list_Character_Positions, list_Words_To_Hide, word_Matching_Thresh_Value\n",
    "    \n",
    "    Predicted_Text = ''\n",
    "    temp_Index = ''\n",
    "    X=Y=W=H=0\n",
    "    list_Character_Positions = []\n",
    "    list_Words_To_Hide = []\n",
    "    word_Matching_Thresh_Value = 80\n",
    "    words_To_Hide = Words_To_Hide\n",
    "    \n",
    "    for line_Area in contours:\n",
    "        x,y,w,h = cv2.boundingRect(line_Area)\n",
    "        X,Y,W,H = x,y,w,h\n",
    "        line_Image = image[y:y+h, x:x+w]\n",
    "        \n",
    "        line_Contours = Get_Countours(line_Image)\n",
    "        line_Contours = Sort_Countours(line_Contours,\"left-to-right\")\n",
    "        \n",
    "        Predicted_Text = Predicted_Text + \" \" + (Get_Text_From_Image(line_Image, line_Contours))\n",
    "    \n",
    "    Predicted_Text = Predicted_Text.strip(\" \")\n",
    "    \n",
    "    json_list_Character_Positions = json.dumps(list_Character_Positions)\n",
    "    \n",
    "    #print (json_list_Character_Positions)\n",
    "    \n",
    "    if not words_To_Hide == \"\":\n",
    "        while \"  \" in words_To_Hide:\n",
    "            words_To_Hide =  words_To_Hide.replace(\"  \",\" \")\n",
    "        if words_To_Hide == \" \":\n",
    "            return json_list_Character_Positions\n",
    "        \n",
    "        \n",
    "        list_Words_To_Hide = words_To_Hide.upper().strip().split(\" \")\n",
    "        list_Similar_Words = Get_Similar_List_Of_Words_To_Hide(list_Words_To_Hide, Predicted_Text)\n",
    "        \n",
    "        for similar_Word in list_Similar_Words:\n",
    "            if not similar_Word in list_Words_To_Hide:\n",
    "                list_Words_To_Hide.append(similar_Word)\n",
    "        \n",
    "        \n",
    "        list_Dimension_Of_Words_Hide = Get_List_Dimension_Of_Words_Hide(Predicted_Text, list_Character_Positions,  list_Words_To_Hide)\n",
    "        \n",
    "        json_list_Dimension_Of_Words_Hide = json.dumps(list_Dimension_Of_Words_Hide)\n",
    "        \n",
    "        return json_list_Character_Positions, json_list_Dimension_Of_Words_Hide\n",
    "    \n",
    "    else:\n",
    "        return json_list_Character_Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1245, 171, 1782, 251, \"HEMANT\"], [1865, 379, 2383, 458, \"MASTEK\"], [827, 376, 1474, 457, \"WORKING\"], [1352, 797, 1947, 888, \"WORKING\"], [438, 1295, 963, 1395, \"PROIECT\"], [1890, 181, 2414, 265, \"SMELAR\"]]\n"
     ]
    }
   ],
   "source": [
    "words_To_Hide = \"  Hemant  MASTEK  Project    Shelar    Working  kiosk\"\n",
    "a, b = (Get_Character_Position_From_Image (r'C:\\Users\\sachin13390\\Desktop\\Self-Declaration.jpg',words_To_Hide))\n",
    "\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
