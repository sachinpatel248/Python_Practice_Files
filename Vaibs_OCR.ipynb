{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sachin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from  scipy import ndimage\n",
    "import math\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "from math import atan2, pi\n",
    "import string\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from keras.models import Sequential, load_model\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "from nltk.tag import pos_tag\n",
    "import json\n",
    "#from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFilePath =r'C:\\Users\\sachin\\Desktop\\f1.png'\n",
    "\n",
    "KERAS_Alphabet_model_File_Path = r'C:\\Users\\sachin\\Desktop\\Keras_Alphabet_Model.h5'\n",
    "Alphabet_Mapping_List = list(string.ascii_uppercase)\n",
    "kernel = np.ones((2,2),np.uint8)\n",
    "index = 0\n",
    "global counts,X,Y,W,H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERAS_Alphabet_Model = load_model(KERAS_Alphabet_model_File_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Processed_Image(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    blur = cv2.GaussianBlur(gray_image,(3,3),0)\n",
    "    ret3,Otsu_Threshold_GaussianBlur_Image = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    Otsu_Threshold_GaussianBlur_Image = cv2.bitwise_not(Otsu_Threshold_GaussianBlur_Image)\n",
    "\n",
    "    closing = cv2.morphologyEx(Otsu_Threshold_GaussianBlur_Image, cv2.MORPH_CLOSE, kernel)\n",
    "    #cv2.imwrite(r'C:\\Users\\sachin\\Desktop\\closing.jpg',closing)\n",
    "\n",
    "    return closing\n",
    "\n",
    "\n",
    "\n",
    "def Get_Dilated_Image(image, number):\n",
    "    global index\n",
    "    kernel = np.ones((3,number),np.uint8)\n",
    "    img_dilation = cv2.dilate(image, kernel, iterations=3)\n",
    "    #index = index + 1\n",
    "    #cv2.imwrite(r'C:\\Users\\sachin\\Desktop\\img_dilation_'+str(index)+'.jpg',img_dilation)\n",
    "    return img_dilation\n",
    "        \n",
    "\n",
    "def Get_Countours(input_Image):\n",
    "    temp_image, contours, hierarchy = cv2.findContours(input_Image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    return contours\n",
    "\n",
    "\n",
    "def Sort_Countours(cnts, method=\"left-to-right\"):\n",
    "    # initialize the reverse flag and sort index\n",
    "    reverse = False\n",
    "    i = 0\n",
    "\n",
    "    # handle if we need to sort in reverse\n",
    "    if method == \"right-to-left\" or method == \"bottom-to-top\":\n",
    "        reverse = True\n",
    "\n",
    "    # handle if we are sorting against the y-coordinate rather than\n",
    "    # the x-coordinate of the bounding box\n",
    "    if method == \"top-to-bottom\" or method == \"bottom-to-top\":\n",
    "        i = 1\n",
    "\n",
    "    # construct the list of bounding boxes and sort them from top to\n",
    "    # bottom\n",
    "    boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
    "    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),\n",
    "                                        key=lambda b:b[1][i], reverse=reverse))\n",
    "\n",
    "    # return the list of sorted contours and bounding boxes\n",
    "    return (cnts)\n",
    "\n",
    "def Draw_Contours(Image, Contours):\n",
    "    for cnt in Contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        #print (str(x) + \" - \" + str(y)+ \" - \" + str(w) + \" - \" + str(h))\n",
    "        cv2.rectangle(Image,(x,y),(x+w,y+h),(0,255,255),1)\n",
    "    #cv2.imwrite(r'C:\\Users\\sachin\\Desktop\\croppedrect.jpg',Image)   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getNewResizedImage(input_Image, Image_Size):\n",
    "    height,width = input_Image.shape\n",
    "    #print (height, width)\n",
    "\n",
    "    if width > height:\n",
    "        aspect_Ratio = (float)(width/height)\n",
    "        width = 20\n",
    "        height = round(width/aspect_Ratio)\n",
    "    else:\n",
    "        aspect_Ratio = (float)(height/width)\n",
    "        height = 20\n",
    "        width = round(height/aspect_Ratio)\n",
    "        \n",
    "    input_Image = cv2.resize(input_Image, (width,height), interpolation = cv2.INTER_AREA )\n",
    "    \n",
    "    height,width = input_Image.shape\n",
    "    \n",
    "    number_Of_Column_To_Add = 28-width\n",
    "    temp_Column = np.zeros( (height , int(number_Of_Column_To_Add/2)), dtype = np.uint8)\n",
    "    input_Image = np.append(temp_Column, input_Image, axis=1)\n",
    "    input_Image = np.append(input_Image, temp_Column, axis=1)\n",
    "\n",
    "\n",
    "    height,width = input_Image.shape\n",
    "\n",
    "    number_Of_Row_To_Add = 28-height\n",
    "    temp_Row= np.zeros( (int(number_Of_Row_To_Add/2) , width ), dtype = np.uint8)\n",
    "    input_Image = np.concatenate((temp_Row,input_Image))\n",
    "    input_Image = np.concatenate((input_Image,temp_Row))\n",
    "\n",
    "    return cv2.resize(input_Image, (Image_Size,Image_Size), interpolation = cv2.INTER_AREA )\n",
    "\n",
    "\n",
    "def Get_Average_Space_From_Image_Line(contours):\n",
    "    total_space = 0\n",
    "    number_Of_Character = 0\n",
    "    last_Point = -1\n",
    "    avg_Space = 0\n",
    "    for cnt in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        #print (x,y,w,h)\n",
    "\n",
    "        if w > 30 and h > 30 and x > 0 and y > 0:\n",
    "            if last_Point == -1:\n",
    "                last_Point = 0\n",
    "            else:\n",
    "                total_space = total_space + ( x - last_Point )\n",
    "            last_Point = x + w\n",
    "\n",
    "            number_Of_Character = number_Of_Character + 1\n",
    "\n",
    "    avg_Space = int (total_space/number_Of_Character)\n",
    "\n",
    "    total_space = 0\n",
    "    last_Point = -1\n",
    "    number_Of_Character = 0\n",
    "\n",
    "##    for cnt in contours:\n",
    "##        x,y,w,h = cv2.boundingRect(cnt)\n",
    "##\n",
    "##        if w > 30 and h > 30 and x > 0 and y > 0:\n",
    "##            if last_Point == -1:\n",
    "##                last_Point = 0\n",
    "##            else:\n",
    "##                space = ( (x + w) - last_Point )\n",
    "##                if space < avg_Space:\n",
    "##                    total_space = total_space + space\n",
    "##                    number_Of_Character = number_Of_Character + 1\n",
    "##            last_Point = x + w\n",
    "##            \n",
    "##    avg_Space = int (total_space/number_Of_Character)\n",
    "\n",
    "    return avg_Space\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Get_Text_From_Image(Image, contours):\n",
    "    global count,X,Y,W,H, list_Character_Positions\n",
    "    KERAS_Alphabet_Prediction = ''\n",
    "\n",
    "    total_space = 0\n",
    "    number_Of_Character = 0\n",
    "    last_Point = -1\n",
    "\n",
    "    #avg_Space = Get_Average_Space_From_Image_Line(contours)\n",
    "    last_Point = -1\n",
    "    total_space = 0\n",
    "    #print (avg_Space)\n",
    "    \n",
    "    Word_Dilated_Image = Get_Dilated_Image(Image,10)\n",
    "    Word_Contours = Get_Countours(Word_Dilated_Image)\n",
    "    Word_Contours = Sort_Countours(Word_Contours,\"left-to-right\")\n",
    "    last_Word_Contour_Index = 0\n",
    "    Word_X, Word_Y, Word_W, Word_H = cv2.boundingRect(Word_Contours[last_Word_Contour_Index])\n",
    "    last_Word_Contour_Max_X_Range = Word_X + Word_W\n",
    "    # i.e X + W\n",
    "            \n",
    "\n",
    "    for cnt in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        give_Space = False\n",
    "        #print (x,y,w,h)\n",
    "\n",
    "        # Reject if Contour is not of desired size (too small)\n",
    "        if w > 8 and h > 15 and x > 0 and y > 0:\n",
    "\n",
    "#             ## Spacing based on average space between character\n",
    "#             if last_Point == -1:\n",
    "#                 last_Point = 0\n",
    "#             else:\n",
    "#                 total_space = ( x - last_Point )\n",
    "\n",
    "#             last_Point = x + w\n",
    "\n",
    "#             if total_space > (avg_Space*1.3):\n",
    "#                 give_Space = True\n",
    "            \n",
    "            ## Spacing based on word formation\n",
    "            if x > last_Word_Contour_Max_X_Range:\n",
    "                give_Space = True\n",
    "                last_Word_Contour_Index = last_Word_Contour_Index + 1\n",
    "                Word_X, Word_Y, Word_W, Word_H = cv2.boundingRect(Word_Contours[last_Word_Contour_Index])\n",
    "                last_Word_Contour_Max_X_Range = Word_X + Word_W\n",
    "                # i.e X + W\n",
    "            \n",
    "    \n",
    "                \n",
    "            if give_Space == True:\n",
    "                KERAS_Alphabet_Prediction = KERAS_Alphabet_Prediction + \" \"\n",
    "                list_Character_Positions.append((-1,-1,-1,-1,\" \"))\n",
    "                \n",
    "\n",
    "            resize_image = getNewResizedImage(Image[y:y+h, x:x+w] , 28)\n",
    "            path = r'C:\\Users\\sachin\\Desktop\\Images\\\\' + str(count)+'.png'\n",
    "            cv2.imwrite(path,resize_image)\n",
    "            resize_image = resize_image.flatten()\n",
    "            count = count + 1\n",
    "\n",
    "            temp_Index = int(KERAS_Alphabet_Model.predict_classes(resize_image.reshape(1,784)/255.0)[0])\n",
    "            alphabet_probability = (KERAS_Alphabet_Model.predict_proba(resize_image.reshape(1,784)/255.0))\n",
    "            sort_alphabet_probability = -np.sort(-alphabet_probability)\n",
    "\n",
    "            KERAS_Alphabet_Prediction = KERAS_Alphabet_Prediction + Alphabet_Mapping_List[int(temp_Index)]\n",
    "\n",
    "            list_Character_Positions.append((x+X,y+Y,w,h,str(Alphabet_Mapping_List[int(temp_Index)])))\n",
    "\n",
    "##            if sort_alphabet_probability[0,0] > 0.99:\n",
    "##                KERAS_Alphabet_Prediction = KERAS_Alphabet_Prediction + Alphabet_Mapping_List[int(temp_Index)]\n",
    "##            else:\n",
    "##                alternate_Probable_Alphabet = Alphabet_Mapping_List[int(np.where(alphabet_probability == sort_alphabet_probability[0,1])[1][0])]\n",
    "##                KERAS_Alphabet_Prediction = KERAS_Alphabet_Prediction + \"[\" + Alphabet_Mapping_List[int(temp_Index)] + \",\" + alternate_Probable_Alphabet + \"]\"\n",
    "\n",
    "    list_Character_Positions.append((-1,-1,-1,-1,\" \"))\n",
    "\n",
    "    return KERAS_Alphabet_Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(imageFilePath)\n",
    "image = Get_Processed_Image(image)\n",
    "image_With_Lines = Get_Dilated_Image(image, 50)\n",
    "\n",
    "contours = Get_Countours(image_With_Lines)\n",
    "contours = Sort_Countours(contours, \"top-to-bottom\")\n",
    "\n",
    "Draw_Contours(cv2.imread(imageFilePath),contours)\n",
    "Predicted_Text = ''\n",
    "temp_Index = ''\n",
    "global list_Character_Positions\n",
    "X=Y=W=H=0\n",
    "count = 20\n",
    "list_Character_Positions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMMEUJJAYALAJXN\n",
      "H\n",
      "MM ROHANEJ\n",
      "ML VUYLSOBGYML\n",
      "CO\n",
      "MB MORPET\n",
      "M PUNE\n",
      "M MH\n",
      "DPM BDGLZBA\n",
      "MR OZX ZGSAAN\n",
      "OT SJOZYS JM JM\n",
      "TTTTTTTTTTTTT\n",
      "AWIMLOJM\n",
      "A\n",
      "KERAS_Alphabet_Predict \n",
      " AMMEUJJAYALAJXN H MM ROHANEJ ML VUYLSOBGYML CO MB MORPET M PUNE M MH DPM BDGLZBA MR OZX ZGSAAN OT SJOZYS JM JM TTTTTTTTTTTTT AWIMLOJM A\n"
     ]
    }
   ],
   "source": [
    "counts = 0\n",
    "index = 0\n",
    "Predicted_Text = ''\n",
    "for line_Area in contours:\n",
    "    counts = counts + 1\n",
    "    x,y,w,h = cv2.boundingRect(line_Area)\n",
    "    X,Y,W,H = x,y,w,h\n",
    "    line_Image = image[y:y+h, x:x+w]\n",
    "    path = r'C:\\Users\\sachin\\Desktop\\Images\\\\' + str(counts)+'.png'\n",
    "    cv2.imwrite(path,line_Image)\n",
    "\n",
    "    line_Contours = Get_Countours(line_Image)\n",
    "    line_Contours = Sort_Countours(line_Contours,\"left-to-right\")\n",
    "    #Draw_Contours(line_Image,line_Contours)\n",
    "    \n",
    "    text = (Get_Text_From_Image(line_Image, line_Contours))\n",
    "    print (text)\n",
    "\n",
    "    Predicted_Text = Predicted_Text + \" \" + text\n",
    "\n",
    "\n",
    "print('KERAS_Alphabet_Predict \\n' + Predicted_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
